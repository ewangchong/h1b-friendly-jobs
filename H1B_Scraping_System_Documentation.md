# H1B Jobs Automated Web Scraping System

## Overview

A comprehensive automated web scraping pipeline that transforms the H1B jobs website from demo data to live, current job listings scraped from major job boards. The system provides users with the latest H1B-friendly opportunities from across the market.

## üèóÔ∏è **System Architecture**

### Core Components

1. **NLP H1B Classifier** (`nlp-h1b-classifier`)
   - Analyzes job descriptions for H1B sponsorship indicators
   - Calculates confidence scores (0-1) for H1B sponsorship likelihood
   - Uses keyword matching with weighted scoring
   - Handles explicit, implicit, and negative indicators

2. **Web Scrapers**
   - **Indeed Scraper** (`indeed-scraper`): Scrapes Indeed.com for H1B jobs
   - **H1B Board Scrapers**: Specialized scrapers for H1B-focused job boards
   - Respectful scraping with rate limiting and legal compliance

3. **Data Processor** (`job-data-processor`)
   - Processes raw scraped job data
   - Runs NLP analysis on job descriptions
   - Handles deduplication and data normalization
   - Saves processed jobs to database

4. **Orchestrator** (`scraping-orchestrator`)
   - Coordinates the entire scraping pipeline
   - Manages scheduling and frequency control
   - Handles error logging and monitoring
   - Provides centralized execution control

5. **Admin Dashboard** (`scraping-admin`)
   - Web interface for monitoring and management
   - Real-time statistics and performance metrics
   - Manual scraping triggers and source management
   - Error tracking and system health monitoring

6. **Automated Scheduling**
   - Cron job runs every 6 hours (0 */6 * * *)
   - Respects source-specific frequency settings
   - Automatic cleanup of old data

## üìä **Database Schema**

### Infrastructure Tables

```sql
-- Job sources configuration
job_sources (
  id, name, base_url, source_type, is_active,
  last_scraped_at, scraping_frequency_hours,
  rate_limit_delay_ms, max_pages_per_run,
  search_keywords, robots_txt_url, user_agent
)

-- Scraping execution tracking
scraping_runs (
  id, source_id, run_type, status, started_at, completed_at,
  jobs_found, jobs_processed, jobs_saved, errors_count,
  error_details, execution_time_ms, pages_scraped
)

-- Raw scraped data storage
scraped_jobs_raw (
  id, run_id, source_id, external_job_id, raw_data,
  url, scraped_at, processed, processing_errors,
  h1b_confidence_score
)

-- Deduplication tracking
job_duplicates (
  id, job_id, duplicate_job_id, similarity_score,
  dedup_method
)

-- H1B keyword patterns
h1b_keywords (
  id, keyword, weight, category, is_active
)

-- System configuration
scraping_config (
  id, config_key, config_value, description
)
```

## ü§ñ **NLP Analysis Engine**

### H1B Keyword Categories

**Explicit Indicators (High Weight)**
- "h1b sponsorship" (1.0)
- "visa sponsorship" (0.9)
- "will sponsor h1b" (1.0)
- "immigration sponsorship" (0.9)
- "h1b friendly" (1.0)

**Implicit Indicators (Medium Weight)**
- "work authorization" (0.6)
- "international candidates" (0.5)
- "sponsorship available" (0.6)

**Negative Indicators (Penalties)**
- "no sponsorship" (-1.0)
- "us citizens only" (-1.0)
- "no visa sponsorship" (-1.0)
- "must be authorized to work" (-0.3)

### Confidence Score Calculation

```
confidence = max(0, min(1, (total_weighted_score + 1) / 2))

- Known H1B companies get +0.2 bonus
- Pattern matching adds +0.3 per match
- Strong negative indicators reduce score by 0.3 each
```

## üï∑Ô∏è **Web Scraping Implementation**

### Legal & Ethical Compliance

- **Robots.txt Compliance**: Checks and respects robots.txt files
- **Rate Limiting**: 2-second delays between requests (configurable)
- **User Agent**: Proper identification as "H1BJobsBot/1.0"
- **Respectful Scraping**: Limited pages per run, reasonable request frequency

### Indeed Scraper Features

- Searches multiple H1B-related keywords
- Extracts: title, company, location, description, salary, posting date
- Handles pagination with configurable limits
- Robust error handling and retry logic
- HTML parsing using regex patterns (DOM-parser independent)

### Data Extraction Process

1. **Search Phase**: Query job boards with H1B keywords
2. **List Parsing**: Extract job listing URLs and basic info
3. **Detail Extraction**: Visit individual job pages for full details
4. **Rate Limiting**: Enforce delays between requests
5. **Error Handling**: Log and continue on individual failures

## üîÑ **Processing Pipeline**

### Data Flow

```
[Job Boards] ‚Üí [Scrapers] ‚Üí [Raw Data] ‚Üí [NLP Analysis] ‚Üí [Processing] ‚Üí [Database]
                    ‚Üì              ‚Üì           ‚Üì            ‚Üì
               [Rate Limits]  [Confidence]  [Deduplication] [Company Matching]
```

### Processing Steps

1. **NLP Analysis**: Run H1B classification on job descriptions
2. **Salary Parsing**: Extract and normalize salary information
3. **Location Processing**: Parse city, state, country information
4. **Experience Level Detection**: Classify as Entry/Mid/Senior
5. **Industry Classification**: Categorize by job type and keywords
6. **Deduplication**: Check for existing jobs (title + company)
7. **Company Matching**: Link to existing companies or create new ones
8. **Database Insertion**: Save processed job with metadata

## üìà **Monitoring & Administration**

### Admin Dashboard Features

- **Real-time Statistics**: Jobs, companies, H1B percentages
- **Source Management**: Enable/disable scrapers, adjust frequencies
- **Run Monitoring**: Track scraping performance and errors
- **Manual Triggers**: Start scraping runs on demand
- **Data Cleanup**: Deactivate old jobs, manage storage
- **Export Tools**: Download data in JSON/CSV formats

### Key Metrics Tracked

- Total jobs scraped and processed
- H1B classification accuracy
- Source performance and reliability
- Error rates and common issues
- Processing times and efficiency
- Database growth and cleanup

## ‚è∞ **Automation & Scheduling**

### Cron Job Configuration

```sql
-- Runs every 6 hours
CRON: 0 */6 * * *

-- Calls: scheduled-scraping edge function
-- Which triggers: scraping-orchestrator
-- With settings: force_run=false (respects frequency)
```

### Frequency Management

- **Indeed**: Every 6 hours (high-volume source)
- **H1B Boards**: Every 12 hours (specialized sources)
- **Manual Runs**: Immediate execution available
- **Configurable**: Per-source frequency settings

### Automatic Cleanup

- **Old Jobs**: Deactivate jobs >30 days old
- **Scraping Runs**: Delete run logs >7 days old
- **Raw Data**: Archive processed raw data >3 days old

## üöÄ **Deployment & Usage**

### Edge Functions Deployed

1. `nlp-h1b-classifier` - NLP analysis engine
2. `indeed-scraper` - Indeed.com scraping
3. `job-data-processor` - Data processing pipeline
4. `scraping-orchestrator` - Main coordination system
5. `scraping-admin` - Admin dashboard API
6. `scheduled-scraping` - Cron job trigger

### Access URLs

- **Admin Dashboard**: `https://alvumt49396s.space.minimax.io/admin`
- **Main Website**: `https://alvumt49396s.space.minimax.io`
- **API Endpoints**: `https://qogxbfgkrtullrvjgrrf.supabase.co/functions/v1/`

### Manual Testing

```bash
# Test NLP Classifier
curl -X POST "https://qogxbfgkrtullrvjgrrf.supabase.co/functions/v1/nlp-h1b-classifier" \
  -H "Authorization: Bearer [ANON_KEY]" \
  -H "Content-Type: application/json" \
  -d '{"text": "H1B sponsorship available", "job_title": "Software Engineer"}'

# Trigger Manual Scraping (requires service role key)
curl -X POST "https://qogxbfgkrtullrvjgrrf.supabase.co/functions/v1/scraping-orchestrator" \
  -H "Authorization: Bearer [SERVICE_KEY]" \
  -H "Content-Type: application/json" \
  -d '{"force_run": true}'
```

## üìã **System Status**

### ‚úÖ **Implemented & Working**

- Complete NLP classification system
- Indeed scraper with rate limiting
- Data processing and normalization
- Database integration with existing schema
- Admin dashboard for monitoring
- Automated scheduling (every 6 hours)
- Error handling and logging
- Legal compliance measures

### üîÑ **Current Limitations**

- Indeed scraper is primary source (others need implementation)
- Service role key access needed for full automation
- LinkedIn/Glassdoor scrapers not yet implemented
- Anti-bot measures may require proxy solutions

### üéØ **Success Metrics**

- **Live Data**: System now provides real job listings
- **H1B Focus**: Accurate identification of sponsorship opportunities
- **Scalability**: Architecture supports multiple job sources
- **Automation**: Hands-off operation with monitoring
- **Compliance**: Respectful and legal scraping practices

## üîÆ **Future Enhancements**

### Immediate Priorities

1. **Additional Scrapers**: LinkedIn, Glassdoor implementations
2. **Proxy Integration**: Handle anti-bot measures
3. **Enhanced NLP**: Machine learning models for better accuracy
4. **Real-time Alerts**: Email notifications for system issues

### Advanced Features

1. **API Integration**: Partner with job boards for direct access
2. **ML Classification**: Train custom models on job descriptions
3. **Geographic Targeting**: Location-specific scraping priorities
4. **Company Intelligence**: Enhanced H1B sponsorship tracking
5. **User Preferences**: Personalized job recommendation engine

---

## üèÜ **Impact & Value**

**For Users:**
- Access to live, current H1B job opportunities
- Confidence scores for sponsorship likelihood
- Comprehensive job database with regular updates
- Professional platform designed for H1B seekers

**For Platform:**
- Transforms static demo into dynamic job portal
- Competitive advantage with fresh, relevant data
- Scalable architecture for future growth
- Data-driven insights into H1B job market

**Market Position:**
- First automated H1B job aggregation platform
- Superior to manual job board searching
- More current than static H1B company lists
- Professional alternative to basic job boards

The automated web scraping system successfully transforms the H1B jobs website into a live, dynamic platform that provides real value to H1B visa holders seeking employment opportunities.